{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13636796,"sourceType":"datasetVersion","datasetId":8667935},{"sourceId":13842226,"sourceType":"datasetVersion","datasetId":8815621},{"sourceId":13842331,"sourceType":"datasetVersion","datasetId":8795576},{"sourceId":649792,"sourceType":"modelInstanceVersion","modelInstanceId":490226,"modelId":505658},{"sourceId":658221,"sourceType":"modelInstanceVersion","modelInstanceId":497634,"modelId":512956}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport time, random\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torchvision.models import vgg16, VGG16_Weights","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.469028Z","iopub.execute_input":"2025-11-23T19:22:36.46934Z","iopub.status.idle":"2025-11-23T19:22:36.474274Z","shell.execute_reply.started":"2025-11-23T19:22:36.469319Z","shell.execute_reply":"2025-11-23T19:22:36.473398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass VGG16(nn.Module):\n    def __init__(self, vgg_path=\"/kaggle/input/vgg-pretrained/pytorch/default/1/vgg16-00b39a1b.pth\"):\n        super(VGG16, self).__init__()\n        vgg16_features = vgg16(weights=None)\n        state = torch.load(vgg_path, map_location='cpu', weights_only=False)\n\n        vgg16_features.load_state_dict(state, strict=False)\n        self.features = vgg16_features.features\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        layers = {'3': 'relu1_2', \n                  '8': 'relu2_2', \n                  '15': 'relu3_3', \n                  '22': 'relu4_3'}\n        \n        features = {}\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in layers:\n                features[layers[name]] = x\n                if (name=='22'):\n                    break\n\n        return features ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.47579Z","iopub.execute_input":"2025-11-23T19:22:36.476038Z","iopub.status.idle":"2025-11-23T19:22:36.488986Z","shell.execute_reply.started":"2025-11-23T19:22:36.476021Z","shell.execute_reply":"2025-11-23T19:22:36.488414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, c):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(c, c, 3, padding='same'),\n            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n            nn.ReLU(),\n            nn.Conv2d(c, c, 3, padding='same'),\n            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass TransformerNetModern(nn.Module):\n\n    # Thay Ä‘á»•i giÃ¡ trá»‹ tanh (cÃ ng cao cÃ ng sÃ¡ng) \n    def __init__(self, tanh_multiplier=180.0):\n        super().__init__()\n\n        # Hoáº·c tÄƒng sá»‘ channels \n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 9, stride=1, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        \n        # TÄƒng sá»‘ residual block á»«  5 -> 8 \n        self.resblocks = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n        # THAY Äá»”I DECODER Äá»‚ TRÃNH CHECKERBOARD\n        self.decoder = nn.Sequential(\n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(128, 64, 3, stride=1, padding=0),\n            nn.InstanceNorm2d(64, affine=True, track_running_stats=False),\n            nn.ReLU(),\n            \n            nn.Upsample(scale_factor=2, mode='nearest'),\n            nn.ReflectionPad2d(1),\n            nn.Conv2d(64, 32, 3, stride=1, padding=0),\n            nn.InstanceNorm2d(32, affine=True, track_running_stats=False),\n            nn.ReLU(),\n            \n            nn.ReflectionPad2d(4),\n            nn.Conv2d(32, 3, 9, stride=1, padding=0),\n            nn.Tanh()\n        )\n        self.tanh_multiplier = tanh_multiplier\n\n    def forward(self, x):\n        out = self.decoder(self.resblocks(self.encoder(x))) * self.tanh_multiplier\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.4898Z","iopub.execute_input":"2025-11-23T19:22:36.49006Z","iopub.status.idle":"2025-11-23T19:22:36.507154Z","shell.execute_reply.started":"2025-11-23T19:22:36.490036Z","shell.execute_reply":"2025-11-23T19:22:36.506431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def gram(tensor):\n    B, C, H, W = tensor.shape\n    x = tensor.view(B, C, H*W)\n    x_t = x.transpose(1, 2)\n    return  torch.bmm(x, x_t) / (C*H*W)\n\ndef load_image(path):\n    # Images loaded as BGR\n    img = cv2.imread(path)\n    return img\n\ndef saveimg(img, image_path):\n    img = img.clip(0, 255)\n    cv2.imwrite(image_path, img)\n\ndef itot(img, max_size=None):\n    if (max_size==None):\n        itot_t = transforms.Compose([\n            #transforms.ToPILImage(),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.mul(255))\n        ])    \n    else:\n        H, W, C = img.shape\n        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n        itot_t = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.mul(255))\n        ])\n\n    tensor = itot_t(img)\n    tensor = tensor.unsqueeze(dim=0)\n    return tensor\n\ndef ttoi(tensor):\n    # Add the means\n    #ttoi_t = transforms.Compose([\n    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n\n    # Remove the batch_size dimension\n    tensor = tensor.squeeze()\n    #img = ttoi_t(tensor)\n    img = tensor.cpu().numpy()\n    \n    # Transpose from [C, H, W] -> [H, W, C]\n    img = img.transpose(1, 2, 0)\n    return img\n\ndef plot_loss_hist(c_loss, s_loss, total_loss, title=\"Loss History\", save_dir=\"/kaggle/working/\"):\n    os.makedirs(save_dir, exist_ok=True)\n    x = [i for i in range(len(total_loss))]\n    plt.figure(figsize=[10, 6])\n    plt.plot(x, c_loss, label=\"Content Loss\")\n    plt.plot(x, s_loss, label=\"Style Loss\")\n    plt.plot(x, total_loss, label=\"Total Loss\")\n    plt.legend()\n    plt.xlabel('Every 500 iterations')\n    plt.ylabel('Loss')\n    plt.title(title)\n    plt.grid(True, linestyle='--', alpha=0.6)  \n\n    save_path = os.path.join(save_dir, f\"{title.replace(' ', '_').lower()}.png\")\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.show()\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.507929Z","iopub.execute_input":"2025-11-23T19:22:36.508568Z","iopub.status.idle":"2025-11-23T19:22:36.526386Z","shell.execute_reply.started":"2025-11-23T19:22:36.508546Z","shell.execute_reply":"2025-11-23T19:22:36.525856Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"TRAIN_IMAGE_SIZE = 256\nDATASET_PATH = \"/kaggle/input/dataset\"\nNUM_EPOCHS = 4\nSTYLE_IMAGE_PATH = \"/kaggle/input/styleimage/la_muse.jpg\"\nBATCH_SIZE = 8 \nCONTENT_WEIGHT = 17 \nSTYLE_WEIGHT = 50 \nADAM_LR = 0.001\nSAVE_MODEL_PATH = \"/kaggle/working/models/\"\nSAVE_IMAGE_PATH = \"/kaggle/working/images/\"\nSAVE_MODEL_EVERY = 1000\nSEED = 35\nPLOT_LOSS = 1\n\ndef train():\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n    os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n\n    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize(TRAIN_IMAGE_SIZE),\n        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255))\n    ])\n    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    TransformerNetwork = TransformerNetModern().to(device)\n    VGG = VGG16().to(device)\n\n    MSELoss = nn.MSELoss().to(device)\n    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n\n    imagenet_neg_mean = torch.tensor(\n        [-103.939, -116.779, -123.68], \n        dtype=torch.float32).reshape(1,3,1,1).to(device)\n    \n    style_image = load_image(STYLE_IMAGE_PATH)\n    style_tensor = itot(style_image).to(device)\n     \n# --------------------------------------------------------\n    # Sá»¬A Lá»–I OOM Táº I ÄÃ‚Y: TÃ­nh Style Features CHá»ˆ Vá»šI BATCH SIZE = 1\n    # --------------------------------------------------------\n    with torch.no_grad(): # Táº¯t tÃ­nh gradient Ä‘á»ƒ tiáº¿t kiá»‡m VRAM vÃ  tÄƒng tá»‘c\n        style_tensor_norm = style_tensor.add(imagenet_neg_mean) # Norm hÃ³a\n        \n        # Chá»‰ Ä‘Æ°a tensor style vá»›i kÃ­ch thÆ°á»›c [1, C, H, W] vÃ o VGG\n        style_features_single = VGG(style_tensor_norm)\n    \n    # TÃ­nh Gram Matrix vÃ  lÆ°u trá»¯\n    style_gram = {}\n    for key, value in style_features_single.items():\n        # Gram Matrix Ä‘Ã£ Ä‘Æ°á»£c tÃ­nh tá»« batch size 1\n        style_gram[key] = gram(value)\n\n    \n    content_loss_history = []\n    style_loss_history = []\n    total_loss_history = []\n    batch_content_loss_sum = 0\n    batch_style_loss_sum = 0\n    batch_total_loss_sum = 0\n\n    batch_count = 1\n    start_time = time.time()\n    for epoch in range(NUM_EPOCHS):\n        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n        for content_batch, _ in train_loader:\n            curr_batch_size = content_batch.shape[0]\n            torch.cuda.empty_cache()\n            optimizer.zero_grad()\n\n            content_batch = content_batch[:,[2,1,0]].to(device)\n            generated_batch = TransformerNetwork(content_batch)\n            content_features = VGG(content_batch.add(imagenet_neg_mean))\n            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n\n            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n            batch_content_loss_sum += content_loss.item()\n\n            style_loss = 0.0\n            for key, value in generated_features.items():\n                \n                # Sá»­a Ä‘á»•i: Má»Ÿ rá»™ng Gram Matrix style (Batch size = 1) \n                # Ä‘á»ƒ khá»›p vá»›i Batch size hiá»‡n táº¡i\n                style_target = style_gram[key].expand(curr_batch_size, -1, -1)\n                \n                s_loss = MSELoss(gram(value), style_target)\n                style_loss += s_loss\n            style_loss *= STYLE_WEIGHT\n\n            \n            batch_style_loss_sum += style_loss.item()\n\n            total_loss = content_loss + style_loss\n            batch_total_loss_sum += total_loss.item()\n\n            total_loss.backward()\n            optimizer.step()\n\n            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n\n                # Save Model\n                checkpoint_path = os.path.join(\n                    SAVE_MODEL_PATH, f\"checkpoint_{batch_count-1}.pth\"\n                )\n                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n\n                # Save sample generated image\n                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n                sample_image = ttoi(sample_tensor.clone().detach())\n                sample_image_path = os.path.join(\n                    SAVE_IMAGE_PATH, f\"sample0_{batch_count-1}.png\"\n                )\n                saveimg(sample_image, sample_image_path)\n                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n\n                content_loss_history.append(batch_content_loss_sum/batch_count)\n                style_loss_history.append(batch_style_loss_sum/batch_count)\n                total_loss_history.append(batch_total_loss_sum/batch_count)\n\n            batch_count+=1\n\n    stop_time = time.time()\n    print(\"Done Training the Transformer Network!\")\n    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n    print(\"========Content Loss========\")\n    print(content_loss_history) \n    print(\"========Style Loss========\")\n    print(style_loss_history) \n    print(\"========Total Loss========\")\n    print(total_loss_history) \n\n    TransformerNetwork.eval()\n    TransformerNetwork.cpu()\n    final_path = os.path.join(SAVE_MODEL_PATH, \"transformer_weight.pth\")\n    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n    torch.save(TransformerNetwork.state_dict(), final_path)\n    print(\"Done saving final model\")\n\n    if (PLOT_LOSS):\n        plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.577308Z","iopub.execute_input":"2025-11-23T19:22:36.577747Z","iopub.status.idle":"2025-11-23T19:22:36.593014Z","shell.execute_reply.started":"2025-11-23T19:22:36.577731Z","shell.execute_reply":"2025-11-23T19:22:36.592175Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" \ndef display_image(img, title=\"Stylized Image\"):\n    \"\"\"\n    Hiá»ƒn thá»‹ áº£nh (NumPy array [H, W, C]) trá»±c tiáº¿p trong Notebook.\n    \"\"\"\n    # 1. Äáº£m báº£o áº£nh náº±m trong dáº£i giÃ¡ trá»‹ [0, 1] cho Matplotlib (náº¿u áº£nh cá»§a báº¡n lÃ  0-255, báº¡n cáº§n chia)\n    # Giáº£ Ä‘á»‹nh ttoi() cá»§a báº¡n tráº£ vá» áº£nh 0-255 (nhÆ° code gá»‘c)\n    if img.max() > 1.0:\n        img = img / 255.0 \n        \n    # 2. Xá»­ lÃ½ Ä‘á»‹nh dáº¡ng mÃ u: Matplotlib máº·c Ä‘á»‹nh lÃ  RGB. \n    # Náº¿u áº£nh Ä‘áº§u ra cá»§a báº¡n lÃ  BGR (do OpenCV), cáº§n chuyá»ƒn BGR -> RGB.\n    # Trong Style Transfer thÆ°á»ng cáº§n chuyá»ƒn: [B, G, R] -> [R, G, B]\n    img = img[:, :, [2, 1, 0]] \n\n    plt.figure(figsize=(8, 8))\n    plt.imshow(img)\n    plt.title(title)\n    plt.axis('off') # áº¨n trá»¥c tá»a Ä‘á»™\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.594178Z","iopub.execute_input":"2025-11-23T19:22:36.594379Z","iopub.status.idle":"2025-11-23T19:22:36.609605Z","shell.execute_reply.started":"2025-11-23T19:22:36.594364Z","shell.execute_reply":"2025-11-23T19:22:36.608849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def stylize_image(content_image_path, output_image_path=None, display_only=True): \n    \"\"\"\n    Stylize má»™t áº£nh Ä‘áº§u vÃ o báº±ng mÃ´ hÃ¬nh Ä‘Ã£ train\n    \n    Args:\n        content_image_path: ÄÆ°á»ng dáº«n áº£nh input\n        output_image_path: TÃªn file output (optional)\n        display_only: Náº¿u True, chá»‰ hiá»ƒn thá»‹ khÃ´ng lÆ°u file\n    \"\"\"\n    \n    # Thiáº¿t láº­p device (pháº£i giá»‘ng device Ä‘Ã£ dÃ¹ng khi train)\n    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # 1. Khá»Ÿi táº¡o mÃ´ hÃ¬nh Transformer (cáº¥u trÃºc rá»—ng)\n    model = TransformerNetModern().to(device)\n    \n    # âŒ FIXED: ÄÆ°á»ng dáº«n bá»‹ trÃ¹ng láº·p\n    # ÄÆ°á»ng dáº«n file trá»ng sá»‘ cuá»‘i cÃ¹ng\n    FINAL_MODEL_PATH = \"/kaggle/input/transform-eight/pytorch/default/1/rabbit_weight.pth\"\n    \n    # 2. Táº£i trá»ng sá»‘ Ä‘Ã£ huáº¥n luyá»‡n\n    print(\"\\n\" + \"=\"*50)\n    print(f\"Loading model weights from:\\n{FINAL_MODEL_PATH}\")\n    \n    try:\n        # Táº£i state dictionary\n        state_dict = torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=False)\n        \n        # Loáº¡i bá» prefix 'module.' náº¿u model Ä‘Æ°á»£c train vá»›i DataParallel\n        # from collections import OrderedDict\n        # new_state_dict = OrderedDict()\n        # for k, v in state_dict.items():\n        #     name = k[7:] if k.startswith('module.') else k\n        #     new_state_dict[name] = v\n        # state_dict = new_state_dict\n        \n        model.load_state_dict(state_dict, strict=True)\n        print(\"âœ… Model weights loaded successfully!\")\n        \n    except FileNotFoundError:\n        print(\"âŒ ERROR: Model weights not found!\")\n        print(f\"Looking for: {FINAL_MODEL_PATH}\")\n        return None\n    except Exception as e:\n        print(f\"âŒ ERROR loading model: {e}\")\n        return None\n    \n    # Chuyá»ƒn sang cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ (Ráº¤T QUAN TRá»ŒNG)\n    model.eval() \n    print(\"Model set to evaluation mode\")\n    \n    # 3. Táº£i vÃ  tiá»n xá»­ lÃ½ áº£nh ná»™i dung\n    print(f\"\\nLoading content image from:\\n{content_image_path}\")\n    \n    try:\n        content_image = load_image(content_image_path)\n        print(f\"Original image shape: {content_image.shape}\")\n    except Exception as e:\n        print(f\"âŒ ERROR loading image: {e}\")\n        return None\n    \n    # Chuyá»ƒn sang Tensor (giá»¯ nguyÃªn kÃ­ch thÆ°á»›c gá»‘c)\n    content_tensor = itot(content_image).to(device)\n    \n    # âŒ FIXED: Training code dÃ¹ng BGR (OpenCV format)\n    # Chuyá»ƒn BGR -> RGB cho Ä‘Ãºng vá»›i training\n    content_tensor = content_tensor[:, [2, 1, 0], :, :]\n    \n    print(f\"Input tensor shape: {content_tensor.shape}\")\n    \n    # 4. Cháº¡y Suy luáº­n (Inference)\n    print(\"\\n\" + \"=\"*50)\n    print(\"Running style transfer...\")\n    \n    with torch.no_grad():\n        start_time = time.time()\n        stylized_tensor = model(content_tensor)\n        end_time = time.time()\n    \n    print(f\"âœ… Style transfer completed in {end_time - start_time:.4f} seconds\")\n    print(f\"Output tensor shape: {stylized_tensor.shape}\")\n    \n    # 5. Háº­u xá»­ lÃ½\n    stylized_image = ttoi(stylized_tensor.detach())\n    \n    # Clip giÃ¡ trá»‹ vá» [0, 255]\n    stylized_image = stylized_image.clip(0, 255).astype(np.uint8)\n    \n    # 6. Hiá»ƒn thá»‹ áº£nh trong notebook\n    print(\"\\n\" + \"=\"*50)\n    print(\"Displaying results...\")\n    \n    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n    \n    # Hiá»ƒn thá»‹ áº£nh gá»‘c (convert BGR to RGB for display)\n    axes[0].imshow(cv2.cvtColor(content_image, cv2.COLOR_BGR2RGB))\n    axes[0].set_title('Original Content Image', fontsize=14, fontweight='bold')\n    axes[0].axis('off')\n    \n    # Hiá»ƒn thá»‹ áº£nh Ä‘Ã£ stylize (Ä‘Ã£ lÃ  RGB)\n    axes[1].imshow(cv2.cvtColor(stylized_image.astype(np.uint8), cv2.COLOR_BGR2RGB))\n    axes[1].set_title('Stylized Image', fontsize=14, fontweight='bold')\n    axes[1].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n    \n    # 7. LÆ°u áº£nh (optional)\n    if not display_only and output_image_path:\n        os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n        final_output_path = os.path.join(SAVE_IMAGE_PATH, output_image_path)\n        saveimg(stylized_image, final_output_path)\n        print(f\"ðŸ’¾ Stylized image saved to: {final_output_path}\")\n    \n    print(\"=\"*50)\n    \n    return stylized_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.610524Z","iopub.execute_input":"2025-11-23T19:22:36.610934Z","iopub.status.idle":"2025-11-23T19:22:36.624353Z","shell.execute_reply.started":"2025-11-23T19:22:36.610908Z","shell.execute_reply":"2025-11-23T19:22:36.623647Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Stylize nhiá»u áº£nh cÃ¹ng lÃºc\ndef stylize_batch(image_paths):\n    \"\"\"Stylize nhiá»u áº£nh vÃ  hiá»ƒn thá»‹ grid\"\"\"\n    results = []\n    \n    for img_path in image_paths:\n        print(f\"\\n{'='*60}\")\n        print(f\"Processing: {img_path}\")\n        print('='*60)\n        result = stylize_image(img_path, display_only=True)\n        if result is not None:\n            results.append(result)\n    \n    # Hiá»ƒn thá»‹ táº¥t cáº£ káº¿t quáº£ trong má»™t grid\n    if results:\n        n_images = len(results)\n        cols = min(3, n_images)\n        rows = (n_images + cols - 1) // cols\n        \n        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n        axes = axes.flatten() if n_images > 1 else [axes]\n        \n        for idx, img in enumerate(results):\n            axes[idx].imshow(cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB))\n            axes[idx].set_title(f'Stylized {idx+1}', fontsize=12)\n            axes[idx].axis('off')\n        \n        # Hide unused subplots\n        for idx in range(n_images, len(axes)):\n            axes[idx].axis('off')\n        \n        plt.tight_layout()\n        plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.625276Z","iopub.execute_input":"2025-11-23T19:22:36.625521Z","iopub.status.idle":"2025-11-23T19:22:36.64106Z","shell.execute_reply.started":"2025-11-23T19:22:36.625481Z","shell.execute_reply":"2025-11-23T19:22:36.640355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_list = [\n     \"/kaggle/input/test-images/baseball.jpg\",\n     \"/kaggle/input/test-images/Anh-chan-dung-nam.jpg\",\n     \"/kaggle/input/test-images/children.jpg\",\n    \"/kaggle/input/test-images/cycling.jpg\",\n    \"/kaggle/input/test-images/Lion.jpg\",\n    \"/kaggle/input/test-images/the-gate.jpg\",\n    \"/kaggle/input/test-images/hoover.jpg\"\n ]\nstylize_batch(image_list)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T19:22:36.642452Z","iopub.execute_input":"2025-11-23T19:22:36.64267Z","iopub.status.idle":"2025-11-23T19:22:44.167555Z","shell.execute_reply.started":"2025-11-23T19:22:36.642656Z","shell.execute_reply":"2025-11-23T19:22:44.16613Z"}},"outputs":[],"execution_count":null}]}