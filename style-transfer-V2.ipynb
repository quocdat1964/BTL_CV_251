{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.46934Z",
     "iopub.status.busy": "2025-11-23T19:22:36.469028Z",
     "iopub.status.idle": "2025-11-23T19:22:36.474274Z",
     "shell.execute_reply": "2025-11-23T19:22:36.473398Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.469319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.476038Z",
     "iopub.status.busy": "2025-11-23T19:22:36.47579Z",
     "iopub.status.idle": "2025-11-23T19:22:36.488986Z",
     "shell.execute_reply": "2025-11-23T19:22:36.488414Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.476021Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, vgg_path=\"/kaggle/input/vgg-pretrained/pytorch/default/1/vgg16-00b39a1b.pth\"):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg16_features = vgg16(weights=None)\n",
    "        state = torch.load(vgg_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "        vgg16_features.load_state_dict(state, strict=False)\n",
    "        self.features = vgg16_features.features\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = {'3': 'relu1_2', \n",
    "                  '8': 'relu2_2', \n",
    "                  '15': 'relu3_3', \n",
    "                  '22': 'relu4_3'}\n",
    "        \n",
    "        features = {}\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "                if (name=='22'):\n",
    "                    break\n",
    "\n",
    "        return features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.49006Z",
     "iopub.status.busy": "2025-11-23T19:22:36.4898Z",
     "iopub.status.idle": "2025-11-23T19:22:36.507154Z",
     "shell.execute_reply": "2025-11-23T19:22:36.506431Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.490036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(c, c, 3, padding='same'),\n",
    "            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c, c, 3, padding='same'),\n",
    "            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class TransformerNetModern(nn.Module):\n",
    "\n",
    "    # Thay đổi giá trị tanh (càng cao càng sáng) \n",
    "    def __init__(self, tanh_multiplier=180.0):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hoặc tăng số channels \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 9, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        # Tăng số residual block ừ  5 -> 8 \n",
    "        self.resblocks = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n",
    "        # THAY ĐỔI DECODER ĐỂ TRÁNH CHECKERBOARD\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(64, affine=True, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=0),\n",
    "            nn.InstanceNorm2d(32, affine=True, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.ReflectionPad2d(4),\n",
    "            nn.Conv2d(32, 3, 9, stride=1, padding=0),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.tanh_multiplier = tanh_multiplier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(self.resblocks(self.encoder(x))) * self.tanh_multiplier\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.508568Z",
     "iopub.status.busy": "2025-11-23T19:22:36.507929Z",
     "iopub.status.idle": "2025-11-23T19:22:36.526386Z",
     "shell.execute_reply": "2025-11-23T19:22:36.525856Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.508546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(B, C, H*W)\n",
    "    x_t = x.transpose(1, 2)\n",
    "    return  torch.bmm(x, x_t) / (C*H*W)\n",
    "\n",
    "def load_image(path):\n",
    "    # Images loaded as BGR\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "def saveimg(img, image_path):\n",
    "    img = img.clip(0, 255)\n",
    "    cv2.imwrite(image_path, img)\n",
    "\n",
    "def itot(img, max_size=None):\n",
    "    if (max_size==None):\n",
    "        itot_t = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])    \n",
    "    else:\n",
    "        H, W, C = img.shape\n",
    "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
    "        itot_t = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "\n",
    "    tensor = itot_t(img)\n",
    "    tensor = tensor.unsqueeze(dim=0)\n",
    "    return tensor\n",
    "\n",
    "def ttoi(tensor):\n",
    "    # Remove the batch_size dimension\n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def plot_loss_hist(c_loss, s_loss, total_loss, title=\"Loss History\", save_dir=\"/kaggle/working/\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    x = [i for i in range(len(total_loss))]\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.plot(x, c_loss, label=\"Content Loss\")\n",
    "    plt.plot(x, s_loss, label=\"Style Loss\")\n",
    "    plt.plot(x, total_loss, label=\"Total Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Every 500 iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)  \n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.577747Z",
     "iopub.status.busy": "2025-11-23T19:22:36.577308Z",
     "iopub.status.idle": "2025-11-23T19:22:36.593014Z",
     "shell.execute_reply": "2025-11-23T19:22:36.592175Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.577731Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_SIZE = 256\n",
    "DATASET_PATH = \"/kaggle/input/dataset\"\n",
    "NUM_EPOCHS = 4\n",
    "STYLE_IMAGE_PATH = \"/kaggle/input/styleimage/la_muse.jpg\"\n",
    "BATCH_SIZE = 8 \n",
    "CONTENT_WEIGHT = 17 \n",
    "STYLE_WEIGHT = 50 \n",
    "ADAM_LR = 0.001\n",
    "SAVE_MODEL_PATH = \"/kaggle/working/models/\"\n",
    "SAVE_IMAGE_PATH = \"/kaggle/working/images/\"\n",
    "SAVE_MODEL_EVERY = 1000\n",
    "SEED = 35\n",
    "PLOT_LOSS = 1\n",
    "\n",
    "def train():\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n",
    "    os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    TransformerNetwork = TransformerNetModern().to(device)\n",
    "    VGG = VGG16().to(device)\n",
    "\n",
    "    MSELoss = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
    "\n",
    "    imagenet_neg_mean = torch.tensor(\n",
    "        [-103.939, -116.779, -123.68], \n",
    "        dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    \n",
    "    style_image = load_image(STYLE_IMAGE_PATH)\n",
    "    style_tensor = itot(style_image).to(device)\n",
    "     \n",
    "    with torch.no_grad(): \n",
    "        style_tensor_norm = style_tensor.add(imagenet_neg_mean) # Norm hóa\n",
    "        \n",
    "        # Chỉ đưa tensor style với kích thước [1, C, H, W] vào VGG\n",
    "        style_features_single = VGG(style_tensor_norm)\n",
    "    \n",
    "    # Tính Gram Matrix và lưu trữ\n",
    "    style_gram = {}\n",
    "    for key, value in style_features_single.items():\n",
    "        # Gram Matrix đã được tính từ batch size 1\n",
    "        style_gram[key] = gram(value)\n",
    "\n",
    "    \n",
    "    content_loss_history = []\n",
    "    style_loss_history = []\n",
    "    total_loss_history = []\n",
    "    batch_content_loss_sum = 0\n",
    "    batch_style_loss_sum = 0\n",
    "    batch_total_loss_sum = 0\n",
    "\n",
    "    batch_count = 1\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n",
    "        for content_batch, _ in train_loader:\n",
    "            curr_batch_size = content_batch.shape[0]\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
    "            generated_batch = TransformerNetwork(content_batch)\n",
    "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
    "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
    "\n",
    "            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n",
    "            batch_content_loss_sum += content_loss.item()\n",
    "\n",
    "            style_loss = 0.0\n",
    "            for key, value in generated_features.items():\n",
    "                \n",
    "                style_target = style_gram[key].expand(curr_batch_size, -1, -1)\n",
    "                \n",
    "                s_loss = MSELoss(gram(value), style_target)\n",
    "                style_loss += s_loss\n",
    "            style_loss *= STYLE_WEIGHT\n",
    "\n",
    "            \n",
    "            batch_style_loss_sum += style_loss.item()\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            batch_total_loss_sum += total_loss.item()\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
    "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
    "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
    "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
    "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
    "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
    "\n",
    "                # Save Model\n",
    "                checkpoint_path = os.path.join(\n",
    "                    SAVE_MODEL_PATH, f\"checkpoint_{batch_count-1}.pth\"\n",
    "                )\n",
    "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
    "                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n",
    "\n",
    "                # Save sample generated image\n",
    "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
    "                sample_image = ttoi(sample_tensor.clone().detach())\n",
    "                sample_image_path = os.path.join(\n",
    "                    SAVE_IMAGE_PATH, f\"sample0_{batch_count-1}.png\"\n",
    "                )\n",
    "                saveimg(sample_image, sample_image_path)\n",
    "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
    "\n",
    "                content_loss_history.append(batch_content_loss_sum/batch_count)\n",
    "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
    "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "\n",
    "            batch_count+=1\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print(\"Done Training the Transformer Network!\")\n",
    "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
    "    print(\"========Content Loss========\")\n",
    "    print(content_loss_history) \n",
    "    print(\"========Style Loss========\")\n",
    "    print(style_loss_history) \n",
    "    print(\"========Total Loss========\")\n",
    "    print(total_loss_history) \n",
    "\n",
    "    TransformerNetwork.eval()\n",
    "    TransformerNetwork.cpu()\n",
    "    final_path = os.path.join(SAVE_MODEL_PATH, \"transformer_weight.pth\")\n",
    "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
    "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
    "    print(\"Done saving final model\")\n",
    "\n",
    "    if (PLOT_LOSS):\n",
    "        plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.594379Z",
     "iopub.status.busy": "2025-11-23T19:22:36.594178Z",
     "iopub.status.idle": "2025-11-23T19:22:36.609605Z",
     "shell.execute_reply": "2025-11-23T19:22:36.608849Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.594364Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    " \n",
    "def display_image(img, title=\"Stylized Image\"):\n",
    "    \"\"\"\n",
    "    Hiển thị ảnh (NumPy array [H, W, C]) trực tiếp trong Notebook.\n",
    "    \"\"\"\n",
    "    if img.max() > 1.0:\n",
    "        img = img / 255.0 \n",
    "        \n",
    "    img = img[:, :, [2, 1, 0]] \n",
    "\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.610934Z",
     "iopub.status.busy": "2025-11-23T19:22:36.610524Z",
     "iopub.status.idle": "2025-11-23T19:22:36.624353Z",
     "shell.execute_reply": "2025-11-23T19:22:36.623647Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.610908Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def stylize_image(content_image_path, output_image_path=None, display_only=True): \n",
    "    \"\"\"\n",
    "    Stylize một ảnh đầu vào bằng mô hình đã train\n",
    "    \n",
    "    Args:\n",
    "        content_image_path: Đường dẫn ảnh input\n",
    "        output_image_path: Tên file output (optional)\n",
    "        display_only: Nếu True, chỉ hiển thị không lưu file\n",
    "    \"\"\"\n",
    "    \n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = TransformerNetModern().to(device)\n",
    "    \n",
    "    FINAL_MODEL_PATH = \"/kaggle/input/transform-eight/pytorch/default/1/rabbit_weight.pth\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Loading model weights from:\\n{FINAL_MODEL_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        state_dict = torch.load(FINAL_MODEL_PATH, map_location=device, weights_only=False)\n",
    "        \n",
    "        \n",
    "        model.load_state_dict(state_dict, strict=True)\n",
    "        print(\"Model weights loaded successfully!\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"ERROR: Model weights not found!\")\n",
    "        print(f\"Looking for: {FINAL_MODEL_PATH}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "    model.eval() \n",
    "    print(\"Model set to evaluation mode\")\n",
    "    \n",
    "    print(f\"\\nLoading content image from:\\n{content_image_path}\")\n",
    "    \n",
    "    try:\n",
    "        content_image = load_image(content_image_path)\n",
    "        print(f\"Original image shape: {content_image.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\" ERROR loading image: {e}\")\n",
    "        return None\n",
    "    \n",
    "    content_tensor = itot(content_image).to(device)\n",
    "    \n",
    "    content_tensor = content_tensor[:, [2, 1, 0], :, :]\n",
    "    \n",
    "    print(f\"Input tensor shape: {content_tensor.shape}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Running style transfer...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_time = time.time()\n",
    "        stylized_tensor = model(content_tensor)\n",
    "        end_time = time.time()\n",
    "    \n",
    "    print(f\"Style transfer completed in {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Output tensor shape: {stylized_tensor.shape}\")\n",
    "\n",
    "    stylized_image = ttoi(stylized_tensor.detach())\n",
    "\n",
    "    stylized_image = stylized_image.clip(0, 255).astype(np.uint8)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Displaying results...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    axes[0].imshow(cv2.cvtColor(content_image, cv2.COLOR_BGR2RGB))\n",
    "    axes[0].set_title('Original Content Image', fontsize=14, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    axes[1].imshow(cv2.cvtColor(stylized_image.astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    axes[1].set_title('Stylized Image', fontsize=14, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    if not display_only and output_image_path:\n",
    "        os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n",
    "        final_output_path = os.path.join(SAVE_IMAGE_PATH, output_image_path)\n",
    "        saveimg(stylized_image, final_output_path)\n",
    "        print(f\"Stylized image saved to: {final_output_path}\")\n",
    "    \n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    return stylized_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.625521Z",
     "iopub.status.busy": "2025-11-23T19:22:36.625276Z",
     "iopub.status.idle": "2025-11-23T19:22:36.64106Z",
     "shell.execute_reply": "2025-11-23T19:22:36.640355Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.625481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Stylize nhiều ảnh cùng lúc\n",
    "def stylize_batch(image_paths):\n",
    "    \"\"\"Stylize nhiều ảnh và hiển thị grid\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing: {img_path}\")\n",
    "        print('='*60)\n",
    "        result = stylize_image(img_path, display_only=True)\n",
    "        if result is not None:\n",
    "            results.append(result)\n",
    "    \n",
    "    # Hiển thị tất cả kết quả trong một grid\n",
    "    if results:\n",
    "        n_images = len(results)\n",
    "        cols = min(3, n_images)\n",
    "        rows = (n_images + cols - 1) // cols\n",
    "        \n",
    "        fig, axes = plt.subplots(rows, cols, figsize=(cols*5, rows*5))\n",
    "        axes = axes.flatten() if n_images > 1 else [axes]\n",
    "        \n",
    "        for idx, img in enumerate(results):\n",
    "            axes[idx].imshow(cv2.cvtColor(img.astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "            axes[idx].set_title(f'Stylized {idx+1}', fontsize=12)\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for idx in range(n_images, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-23T19:22:36.64267Z",
     "iopub.status.busy": "2025-11-23T19:22:36.642452Z",
     "iopub.status.idle": "2025-11-23T19:22:44.167555Z",
     "shell.execute_reply": "2025-11-23T19:22:44.16613Z",
     "shell.execute_reply.started": "2025-11-23T19:22:36.642656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_list = [\n",
    "     \"/kaggle/input/test-images/baseball.jpg\",\n",
    "     \"/kaggle/input/test-images/Anh-chan-dung-nam.jpg\",\n",
    "     \"/kaggle/input/test-images/children.jpg\",\n",
    "    \"/kaggle/input/test-images/cycling.jpg\",\n",
    "    \"/kaggle/input/test-images/Lion.jpg\",\n",
    "    \"/kaggle/input/test-images/the-gate.jpg\",\n",
    "    \"/kaggle/input/test-images/hoover.jpg\"\n",
    " ]\n",
    "stylize_batch(image_list)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8667935,
     "sourceId": 13636796,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8815621,
     "sourceId": 13842226,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8795576,
     "sourceId": 13842331,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 505658,
     "modelInstanceId": 490226,
     "sourceId": 649792,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 512956,
     "modelInstanceId": 497634,
     "sourceId": 658221,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
