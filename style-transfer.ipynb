{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13636796,"sourceType":"datasetVersion","datasetId":8667935},{"sourceId":13642835,"sourceType":"datasetVersion","datasetId":8672300},{"sourceId":13659254,"sourceType":"datasetVersion","datasetId":8684300},{"sourceId":13659258,"sourceType":"datasetVersion","datasetId":8684304},{"sourceId":13668833,"sourceType":"datasetVersion","datasetId":8690966},{"sourceId":13775925,"sourceType":"datasetVersion","datasetId":8768129},{"sourceId":633526,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":477658,"modelId":493607}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport numpy as np\nimport os\nimport time, random\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import transforms, datasets\nfrom torchvision.models import vgg16, VGG16_Weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:49:57.145444Z","iopub.execute_input":"2025-11-18T09:49:57.145653Z","iopub.status.idle":"2025-11-18T09:50:03.907455Z","shell.execute_reply.started":"2025-11-18T09:49:57.145636Z","shell.execute_reply":"2025-11-18T09:50:03.906865Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class VGG16(nn.Module):\n    def __init__(self, vgg_path=\"/kaggle/input/vgg-pretrained/pytorch/default/1/vgg16-00b39a1b.pth\"):\n        super(VGG16, self).__init__()\n        vgg16_features = vgg16(weights=None)\n        state = torch.load(vgg_path, map_location='cpu', weights_only=False)\n\n        vgg16_features.load_state_dict(state, strict=False)\n        self.features = vgg16_features.features\n\n        for param in self.features.parameters():\n            param.requires_grad = False\n\n    def forward(self, x):\n        layers = {'3': 'relu1_2', \n                  '8': 'relu2_2', \n                  '15': 'relu3_3', \n                  '22': 'relu4_3'}\n        \n        features = {}\n        for name, layer in self.features._modules.items():\n            x = layer(x)\n            if name in layers:\n                features[layers[name]] = x\n                if (name=='22'):\n                    break\n\n        return features","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:50:03.908150Z","iopub.execute_input":"2025-11-18T09:50:03.908421Z","iopub.status.idle":"2025-11-18T09:50:03.915218Z","shell.execute_reply.started":"2025-11-18T09:50:03.908404Z","shell.execute_reply":"2025-11-18T09:50:03.914584Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    def __init__(self, c):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(c, c, 3, padding='same'),\n            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n            nn.ReLU(),\n            nn.Conv2d(c, c, 3, padding='same'),\n            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n\nclass TransformerNetModern(nn.Module):\n    def __init__(self, tanh_multiplier=150.0):\n        super().__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 32, 9, stride=1, padding='same'),\n            nn.ReLU(),\n            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n            nn.ReLU(),\n        )\n        self.resblocks = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n        self.decoder = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n            nn.ReLU(),\n            nn.Conv2d(32, 3, 9, stride=1, padding='same'),\n            nn.Tanh()\n        )\n        self.tanh_multiplier = tanh_multiplier\n\n    def forward(self, x):\n        out = self.decoder(self.resblocks(self.encoder(x))) * self.tanh_multiplier\n        return out","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:50:03.917284Z","iopub.execute_input":"2025-11-18T09:50:03.917490Z","iopub.status.idle":"2025-11-18T09:50:03.934867Z","shell.execute_reply.started":"2025-11-18T09:50:03.917475Z","shell.execute_reply":"2025-11-18T09:50:03.934254Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def gram(tensor):\n    B, C, H, W = tensor.shape\n    x = tensor.view(B, C, H*W)\n    x_t = x.transpose(1, 2)\n    return  torch.bmm(x, x_t) / (C*H*W)\n\ndef load_image(path):\n    # Images loaded as BGR\n    img = cv2.imread(path)\n    return img\n\ndef saveimg(img, image_path):\n    img = img.clip(0, 255)\n    cv2.imwrite(image_path, img)\n\ndef itot(img, max_size=None):\n    if (max_size==None):\n        itot_t = transforms.Compose([\n            #transforms.ToPILImage(),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.mul(255))\n        ])    \n    else:\n        H, W, C = img.shape\n        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n        itot_t = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Lambda(lambda x: x.mul(255))\n        ])\n\n    tensor = itot_t(img)\n    tensor = tensor.unsqueeze(dim=0)\n    return tensor\n\ndef ttoi(tensor):\n    # Add the means\n    #ttoi_t = transforms.Compose([\n    #    transforms.Normalize([-103.939, -116.779, -123.68],[1,1,1])])\n\n    # Remove the batch_size dimension\n    tensor = tensor.squeeze()\n    #img = ttoi_t(tensor)\n    img = tensor.cpu().numpy()\n    \n    # Transpose from [C, H, W] -> [H, W, C]\n    img = img.transpose(1, 2, 0)\n    return img\n\ndef plot_loss_hist(c_loss, s_loss, total_loss, title=\"Loss History\", save_dir=\"/kaggle/working/\"):\n    os.makedirs(save_dir, exist_ok=True)\n    x = [i for i in range(len(total_loss))]\n    plt.figure(figsize=[10, 6])\n    plt.plot(x, c_loss, label=\"Content Loss\")\n    plt.plot(x, s_loss, label=\"Style Loss\")\n    plt.plot(x, total_loss, label=\"Total Loss\")\n    plt.legend()\n    plt.xlabel('Every 500 iterations')\n    plt.ylabel('Loss')\n    plt.title(title)\n    plt.grid(True, linestyle='--', alpha=0.6)  \n\n    save_path = os.path.join(save_dir, f\"{title.replace(' ', '_').lower()}.png\")\n    plt.savefig(save_path, bbox_inches='tight')\n    plt.show()\n    plt.close()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:50:03.935466Z","iopub.execute_input":"2025-11-18T09:50:03.935708Z","iopub.status.idle":"2025-11-18T09:50:03.953960Z","shell.execute_reply.started":"2025-11-18T09:50:03.935687Z","shell.execute_reply":"2025-11-18T09:50:03.953296Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"TRAIN_IMAGE_SIZE = 256\nDATASET_PATH = \"/kaggle/input/dataset\"\nNUM_EPOCHS = 4\nSTYLE_IMAGE_PATH = \"/kaggle/input/the-scream/Edvard-Munch-The-Scream.jpg\"\nBATCH_SIZE = 8 \nCONTENT_WEIGHT = 17 \nSTYLE_WEIGHT = 50 \nADAM_LR = 0.001\nSAVE_MODEL_PATH = \"/kaggle/working/models/\"\nSAVE_IMAGE_PATH = \"/kaggle/working/images/\"\nSAVE_MODEL_EVERY = 1000\nSEED = 35\nPLOT_LOSS = 1\n\ndef train():\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n    os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n\n    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    transform = transforms.Compose([\n        transforms.Resize(TRAIN_IMAGE_SIZE),\n        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n        transforms.ToTensor(),\n        transforms.Lambda(lambda x: x.mul(255))\n    ])\n    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\n    TransformerNetwork = TransformerNetModern().to(device)\n    VGG = VGG16().to(device)\n\n    MSELoss = nn.MSELoss().to(device)\n    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n\n    imagenet_neg_mean = torch.tensor(\n        [-103.939, -116.779, -123.68], \n        dtype=torch.float32).reshape(1,3,1,1).to(device)\n    \n    style_image = load_image(STYLE_IMAGE_PATH)\n    style_tensor = itot(style_image).to(device)\n    style_tensor = style_tensor.add(imagenet_neg_mean)\n    B, C, H, W = style_tensor.shape\n    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n    style_gram = {}\n    for key, value in style_features.items():\n        style_gram[key] = gram(value)\n\n    content_loss_history = []\n    style_loss_history = []\n    total_loss_history = []\n    batch_content_loss_sum = 0\n    batch_style_loss_sum = 0\n    batch_total_loss_sum = 0\n\n    batch_count = 1\n    start_time = time.time()\n    for epoch in range(NUM_EPOCHS):\n        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n        for content_batch, _ in train_loader:\n            curr_batch_size = content_batch.shape[0]\n            torch.cuda.empty_cache()\n            optimizer.zero_grad()\n\n            content_batch = content_batch[:,[2,1,0]].to(device)\n            generated_batch = TransformerNetwork(content_batch)\n            content_features = VGG(content_batch.add(imagenet_neg_mean))\n            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n\n            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n            batch_content_loss_sum += content_loss.item()\n\n            style_loss = 0.0\n            for key, value in generated_features.items():\n                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n                style_loss += s_loss\n            style_loss *= STYLE_WEIGHT\n            batch_style_loss_sum += style_loss.item()\n\n            total_loss = content_loss + style_loss\n            batch_total_loss_sum += total_loss.item()\n\n            total_loss.backward()\n            optimizer.step()\n\n            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n\n                # Save Model\n                checkpoint_path = os.path.join(\n                    SAVE_MODEL_PATH, f\"checkpoint_{batch_count-1}.pth\"\n                )\n                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n\n                # Save sample generated image\n                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n                sample_image = ttoi(sample_tensor.clone().detach())\n                sample_image_path = os.path.join(\n                    SAVE_IMAGE_PATH, f\"sample0_{batch_count-1}.png\"\n                )\n                saveimg(sample_image, sample_image_path)\n                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n\n                content_loss_history.append(batch_content_loss_sum/batch_count)\n                style_loss_history.append(batch_style_loss_sum/batch_count)\n                total_loss_history.append(batch_total_loss_sum/batch_count)\n\n            batch_count+=1\n\n    stop_time = time.time()\n    print(\"Done Training the Transformer Network!\")\n    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n    print(\"========Content Loss========\")\n    print(content_loss_history) \n    print(\"========Style Loss========\")\n    print(style_loss_history) \n    print(\"========Total Loss========\")\n    print(total_loss_history) \n\n    TransformerNetwork.eval()\n    TransformerNetwork.cpu()\n    final_path = os.path.join(SAVE_MODEL_PATH, \"transformer_weight.pth\")\n    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n    torch.save(TransformerNetwork.state_dict(), final_path)\n    print(\"Done saving final model\")\n\n    if (PLOT_LOSS):\n        plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:50:03.954716Z","iopub.execute_input":"2025-11-18T09:50:03.954939Z","iopub.status.idle":"2025-11-18T09:50:03.973866Z","shell.execute_reply.started":"2025-11-18T09:50:03.954915Z","shell.execute_reply":"2025-11-18T09:50:03.973330Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T09:50:03.974459Z","iopub.execute_input":"2025-11-18T09:50:03.974614Z"}},"outputs":[{"name":"stdout","text":"========Epoch 1/4========\n========Iteration 1/41392========\n\tContent Loss:\t5231780.50\n\tStyle Loss:\t135423712.00\n\tTotal Loss:\t140655488.00\nTime elapsed:\t0.7170536518096924 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_0.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_0.png\n","output_type":"stream"},{"name":"stderr","text":"[ WARN:0@106.580] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n","output_type":"stream"},{"name":"stdout","text":"========Iteration 1001/41392========\n\tContent Loss:\t2248715.04\n\tStyle Loss:\t4402975.79\n\tTotal Loss:\t6651690.76\nTime elapsed:\t288.1394329071045 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_1000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_1000.png\n========Iteration 2001/41392========\n\tContent Loss:\t1937235.20\n\tStyle Loss:\t2899472.34\n\tTotal Loss:\t4836707.50\nTime elapsed:\t575.3273952007294 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_2000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_2000.png\n========Iteration 3001/41392========\n\tContent Loss:\t1777177.39\n\tStyle Loss:\t2380769.65\n\tTotal Loss:\t4157947.02\nTime elapsed:\t862.3278906345367 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_3000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_3000.png\n========Iteration 4001/41392========\n\tContent Loss:\t1676921.56\n\tStyle Loss:\t2115106.05\n\tTotal Loss:\t3792027.59\nTime elapsed:\t1149.2903530597687 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_4000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_4000.png\n========Iteration 5001/41392========\n\tContent Loss:\t1608214.56\n\tStyle Loss:\t1951054.03\n\tTotal Loss:\t3559268.57\nTime elapsed:\t1436.3437678813934 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_5000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_5000.png\n========Iteration 6001/41392========\n\tContent Loss:\t1559053.31\n\tStyle Loss:\t1837787.53\n\tTotal Loss:\t3396840.83\nTime elapsed:\t1723.3368210792542 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_6000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_6000.png\n========Iteration 7001/41392========\n\tContent Loss:\t1519355.93\n\tStyle Loss:\t1754842.20\n\tTotal Loss:\t3274198.13\nTime elapsed:\t2010.6342356204987 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_7000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_7000.png\n========Iteration 8001/41392========\n\tContent Loss:\t1489776.11\n\tStyle Loss:\t1690700.18\n\tTotal Loss:\t3180476.27\nTime elapsed:\t2298.186637878418 seconds\nSaved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_8000.pth\nSaved sample tranformed image at /kaggle/working/images/sample0_8000.png\n","output_type":"stream"}],"execution_count":null}]}