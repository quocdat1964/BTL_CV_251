{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T09:49:57.145653Z",
     "iopub.status.busy": "2025-11-18T09:49:57.145444Z",
     "iopub.status.idle": "2025-11-18T09:50:03.907455Z",
     "shell.execute_reply": "2025-11-18T09:50:03.906865Z",
     "shell.execute_reply.started": "2025-11-18T09:49:57.145636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time, random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import vgg16, VGG16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-18T09:50:03.908421Z",
     "iopub.status.busy": "2025-11-18T09:50:03.908150Z",
     "iopub.status.idle": "2025-11-18T09:50:03.915218Z",
     "shell.execute_reply": "2025-11-18T09:50:03.914584Z",
     "shell.execute_reply.started": "2025-11-18T09:50:03.908404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, vgg_path=\"/kaggle/input/vgg-pretrained/pytorch/default/1/vgg16-00b39a1b.pth\"):\n",
    "        super(VGG16, self).__init__()\n",
    "        vgg16_features = vgg16(weights=None)\n",
    "        state = torch.load(vgg_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "        vgg16_features.load_state_dict(state, strict=False)\n",
    "        self.features = vgg16_features.features\n",
    "\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        layers = {'3': 'relu1_2', \n",
    "                  '8': 'relu2_2', \n",
    "                  '15': 'relu3_3', \n",
    "                  '22': 'relu4_3'}\n",
    "        \n",
    "        features = {}\n",
    "        for name, layer in self.features._modules.items():\n",
    "            x = layer(x)\n",
    "            if name in layers:\n",
    "                features[layers[name]] = x\n",
    "                if (name=='22'):\n",
    "                    break\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T09:50:03.917490Z",
     "iopub.status.busy": "2025-11-18T09:50:03.917284Z",
     "iopub.status.idle": "2025-11-18T09:50:03.934867Z",
     "shell.execute_reply": "2025-11-18T09:50:03.934254Z",
     "shell.execute_reply.started": "2025-11-18T09:50:03.917475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, c):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(c, c, 3, padding='same'),\n",
    "            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(c, c, 3, padding='same'),\n",
    "            nn.InstanceNorm2d(c, affine=True, track_running_stats=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "class TransformerNetModern(nn.Module):\n",
    "    def __init__(self, tanh_multiplier=150.0):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 9, stride=1, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.resblocks = nn.Sequential(*[ResidualBlock(128) for _ in range(5)])\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 9, stride=1, padding='same'),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.tanh_multiplier = tanh_multiplier\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.decoder(self.resblocks(self.encoder(x))) * self.tanh_multiplier\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T09:50:03.935708Z",
     "iopub.status.busy": "2025-11-18T09:50:03.935466Z",
     "iopub.status.idle": "2025-11-18T09:50:03.953960Z",
     "shell.execute_reply": "2025-11-18T09:50:03.953296Z",
     "shell.execute_reply.started": "2025-11-18T09:50:03.935687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def gram(tensor):\n",
    "    B, C, H, W = tensor.shape\n",
    "    x = tensor.view(B, C, H*W)\n",
    "    x_t = x.transpose(1, 2)\n",
    "    return  torch.bmm(x, x_t) / (C*H*W)\n",
    "\n",
    "def load_image(path):\n",
    "    # Images loaded as BGR\n",
    "    img = cv2.imread(path)\n",
    "    return img\n",
    "\n",
    "def saveimg(img, image_path):\n",
    "    img = img.clip(0, 255)\n",
    "    cv2.imwrite(image_path, img)\n",
    "\n",
    "def itot(img, max_size=None):\n",
    "    if (max_size==None):\n",
    "        itot_t = transforms.Compose([\n",
    "            #transforms.ToPILImage(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])    \n",
    "    else:\n",
    "        H, W, C = img.shape\n",
    "        image_size = tuple([int((float(max_size) / max([H,W]))*x) for x in [H, W]])\n",
    "        itot_t = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Lambda(lambda x: x.mul(255))\n",
    "        ])\n",
    "\n",
    "    tensor = itot_t(img)\n",
    "    tensor = tensor.unsqueeze(dim=0)\n",
    "    return tensor\n",
    "\n",
    "def ttoi(tensor):\n",
    "\n",
    "    # Remove the batch_size dimension\n",
    "    tensor = tensor.squeeze()\n",
    "    #img = ttoi_t(tensor)\n",
    "    img = tensor.cpu().numpy()\n",
    "    \n",
    "    # Transpose from [C, H, W] -> [H, W, C]\n",
    "    img = img.transpose(1, 2, 0)\n",
    "    return img\n",
    "\n",
    "def plot_loss_hist(c_loss, s_loss, total_loss, title=\"Loss History\", save_dir=\"/kaggle/working/\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    x = [i for i in range(len(total_loss))]\n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.plot(x, c_loss, label=\"Content Loss\")\n",
    "    plt.plot(x, s_loss, label=\"Style Loss\")\n",
    "    plt.plot(x, total_loss, label=\"Total Loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel('Every 500 iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)  \n",
    "\n",
    "    save_path = os.path.join(save_dir, f\"{title.replace(' ', '_').lower()}.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T09:50:03.954939Z",
     "iopub.status.busy": "2025-11-18T09:50:03.954716Z",
     "iopub.status.idle": "2025-11-18T09:50:03.973866Z",
     "shell.execute_reply": "2025-11-18T09:50:03.973330Z",
     "shell.execute_reply.started": "2025-11-18T09:50:03.954915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_IMAGE_SIZE = 256\n",
    "DATASET_PATH = \"/kaggle/input/dataset\"\n",
    "NUM_EPOCHS = 4\n",
    "STYLE_IMAGE_PATH = \"/kaggle/input/the-scream/Edvard-Munch-The-Scream.jpg\"\n",
    "BATCH_SIZE = 8 \n",
    "CONTENT_WEIGHT = 17 \n",
    "STYLE_WEIGHT = 50 \n",
    "ADAM_LR = 0.001\n",
    "SAVE_MODEL_PATH = \"/kaggle/working/models/\"\n",
    "SAVE_IMAGE_PATH = \"/kaggle/working/images/\"\n",
    "SAVE_MODEL_EVERY = 1000\n",
    "SEED = 35\n",
    "PLOT_LOSS = 1\n",
    "\n",
    "def train():\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.makedirs(SAVE_MODEL_PATH, exist_ok=True)\n",
    "    os.makedirs(SAVE_IMAGE_PATH, exist_ok=True)\n",
    "\n",
    "    device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(TRAIN_IMAGE_SIZE),\n",
    "        transforms.CenterCrop(TRAIN_IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "    ])\n",
    "    train_dataset = datasets.ImageFolder(DATASET_PATH, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    TransformerNetwork = TransformerNetModern().to(device)\n",
    "    VGG = VGG16().to(device)\n",
    "\n",
    "    MSELoss = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(TransformerNetwork.parameters(), lr=ADAM_LR)\n",
    "\n",
    "    imagenet_neg_mean = torch.tensor(\n",
    "        [-103.939, -116.779, -123.68], \n",
    "        dtype=torch.float32).reshape(1,3,1,1).to(device)\n",
    "    \n",
    "    style_image = load_image(STYLE_IMAGE_PATH)\n",
    "    style_tensor = itot(style_image).to(device)\n",
    "    style_tensor = style_tensor.add(imagenet_neg_mean)\n",
    "    B, C, H, W = style_tensor.shape\n",
    "    style_features = VGG(style_tensor.expand([BATCH_SIZE, C, H, W]))\n",
    "    style_gram = {}\n",
    "    for key, value in style_features.items():\n",
    "        style_gram[key] = gram(value)\n",
    "\n",
    "    content_loss_history = []\n",
    "    style_loss_history = []\n",
    "    total_loss_history = []\n",
    "    batch_content_loss_sum = 0\n",
    "    batch_style_loss_sum = 0\n",
    "    batch_total_loss_sum = 0\n",
    "\n",
    "    batch_count = 1\n",
    "    start_time = time.time()\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(\"========Epoch {}/{}========\".format(epoch+1, NUM_EPOCHS))\n",
    "        for content_batch, _ in train_loader:\n",
    "            curr_batch_size = content_batch.shape[0]\n",
    "            torch.cuda.empty_cache()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            content_batch = content_batch[:,[2,1,0]].to(device)\n",
    "            generated_batch = TransformerNetwork(content_batch)\n",
    "            content_features = VGG(content_batch.add(imagenet_neg_mean))\n",
    "            generated_features = VGG(generated_batch.add(imagenet_neg_mean))\n",
    "\n",
    "            content_loss = CONTENT_WEIGHT * MSELoss(generated_features['relu2_2'], content_features['relu2_2'])            \n",
    "            batch_content_loss_sum += content_loss.item()\n",
    "\n",
    "            style_loss = 0.0\n",
    "            for key, value in generated_features.items():\n",
    "                s_loss = MSELoss(gram(value), style_gram[key][:curr_batch_size])\n",
    "                style_loss += s_loss\n",
    "            style_loss *= STYLE_WEIGHT\n",
    "            batch_style_loss_sum += style_loss.item()\n",
    "\n",
    "            total_loss = content_loss + style_loss\n",
    "            batch_total_loss_sum += total_loss.item()\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (((batch_count-1)%SAVE_MODEL_EVERY == 0) or (batch_count==NUM_EPOCHS*len(train_loader))):\n",
    "                print(\"========Iteration {}/{}========\".format(batch_count, NUM_EPOCHS*len(train_loader)))\n",
    "                print(\"\\tContent Loss:\\t{:.2f}\".format(batch_content_loss_sum/batch_count))\n",
    "                print(\"\\tStyle Loss:\\t{:.2f}\".format(batch_style_loss_sum/batch_count))\n",
    "                print(\"\\tTotal Loss:\\t{:.2f}\".format(batch_total_loss_sum/batch_count))\n",
    "                print(\"Time elapsed:\\t{} seconds\".format(time.time()-start_time))\n",
    "\n",
    "                # Save Model\n",
    "                checkpoint_path = os.path.join(\n",
    "                    SAVE_MODEL_PATH, f\"checkpoint_{batch_count-1}.pth\"\n",
    "                )\n",
    "                torch.save(TransformerNetwork.state_dict(), checkpoint_path)\n",
    "                print(\"Saved TransformerNetwork checkpoint file at {}\".format(checkpoint_path))\n",
    "\n",
    "                # Save sample generated image\n",
    "                sample_tensor = generated_batch[0].clone().detach().unsqueeze(dim=0)\n",
    "                sample_image = ttoi(sample_tensor.clone().detach())\n",
    "                sample_image_path = os.path.join(\n",
    "                    SAVE_IMAGE_PATH, f\"sample0_{batch_count-1}.png\"\n",
    "                )\n",
    "                saveimg(sample_image, sample_image_path)\n",
    "                print(\"Saved sample tranformed image at {}\".format(sample_image_path))\n",
    "\n",
    "                content_loss_history.append(batch_content_loss_sum/batch_count)\n",
    "                style_loss_history.append(batch_style_loss_sum/batch_count)\n",
    "                total_loss_history.append(batch_total_loss_sum/batch_count)\n",
    "\n",
    "            batch_count+=1\n",
    "\n",
    "    stop_time = time.time()\n",
    "    print(\"Done Training the Transformer Network!\")\n",
    "    print(\"Training Time: {} seconds\".format(stop_time-start_time))\n",
    "    print(\"========Content Loss========\")\n",
    "    print(content_loss_history) \n",
    "    print(\"========Style Loss========\")\n",
    "    print(style_loss_history) \n",
    "    print(\"========Total Loss========\")\n",
    "    print(total_loss_history) \n",
    "\n",
    "    TransformerNetwork.eval()\n",
    "    TransformerNetwork.cpu()\n",
    "    final_path = os.path.join(SAVE_MODEL_PATH, \"transformer_weight.pth\")\n",
    "    print(\"Saving TransformerNetwork weights at {}\".format(final_path))\n",
    "    torch.save(TransformerNetwork.state_dict(), final_path)\n",
    "    print(\"Done saving final model\")\n",
    "\n",
    "    if (PLOT_LOSS):\n",
    "        plot_loss_hist(content_loss_history, style_loss_history, total_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T09:50:03.974614Z",
     "iopub.status.busy": "2025-11-18T09:50:03.974459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Epoch 1/4========\n",
      "========Iteration 1/41392========\n",
      "\tContent Loss:\t5231780.50\n",
      "\tStyle Loss:\t135423712.00\n",
      "\tTotal Loss:\t140655488.00\n",
      "Time elapsed:\t0.7170536518096924 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_0.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_0.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@106.580] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========Iteration 1001/41392========\n",
      "\tContent Loss:\t2248715.04\n",
      "\tStyle Loss:\t4402975.79\n",
      "\tTotal Loss:\t6651690.76\n",
      "Time elapsed:\t288.1394329071045 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_1000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_1000.png\n",
      "========Iteration 2001/41392========\n",
      "\tContent Loss:\t1937235.20\n",
      "\tStyle Loss:\t2899472.34\n",
      "\tTotal Loss:\t4836707.50\n",
      "Time elapsed:\t575.3273952007294 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_2000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_2000.png\n",
      "========Iteration 3001/41392========\n",
      "\tContent Loss:\t1777177.39\n",
      "\tStyle Loss:\t2380769.65\n",
      "\tTotal Loss:\t4157947.02\n",
      "Time elapsed:\t862.3278906345367 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_3000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_3000.png\n",
      "========Iteration 4001/41392========\n",
      "\tContent Loss:\t1676921.56\n",
      "\tStyle Loss:\t2115106.05\n",
      "\tTotal Loss:\t3792027.59\n",
      "Time elapsed:\t1149.2903530597687 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_4000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_4000.png\n",
      "========Iteration 5001/41392========\n",
      "\tContent Loss:\t1608214.56\n",
      "\tStyle Loss:\t1951054.03\n",
      "\tTotal Loss:\t3559268.57\n",
      "Time elapsed:\t1436.3437678813934 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_5000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_5000.png\n",
      "========Iteration 6001/41392========\n",
      "\tContent Loss:\t1559053.31\n",
      "\tStyle Loss:\t1837787.53\n",
      "\tTotal Loss:\t3396840.83\n",
      "Time elapsed:\t1723.3368210792542 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_6000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_6000.png\n",
      "========Iteration 7001/41392========\n",
      "\tContent Loss:\t1519355.93\n",
      "\tStyle Loss:\t1754842.20\n",
      "\tTotal Loss:\t3274198.13\n",
      "Time elapsed:\t2010.6342356204987 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_7000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_7000.png\n",
      "========Iteration 8001/41392========\n",
      "\tContent Loss:\t1489776.11\n",
      "\tStyle Loss:\t1690700.18\n",
      "\tTotal Loss:\t3180476.27\n",
      "Time elapsed:\t2298.186637878418 seconds\n",
      "Saved TransformerNetwork checkpoint file at /kaggle/working/models/checkpoint_8000.pth\n",
      "Saved sample tranformed image at /kaggle/working/images/sample0_8000.png\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8667935,
     "sourceId": 13636796,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8672300,
     "sourceId": 13642835,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8684300,
     "sourceId": 13659254,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8684304,
     "sourceId": 13659258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8690966,
     "sourceId": 13668833,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8768129,
     "sourceId": 13775925,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 493607,
     "modelInstanceId": 477658,
     "sourceId": 633526,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
